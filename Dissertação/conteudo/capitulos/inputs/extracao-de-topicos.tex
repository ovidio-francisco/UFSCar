
\section{Modelos de Extração de Tópicos}
% Na venda do peixe, citar que a busca é tão difícil quanto a leitura e análise de coleções de documentos.

% Os modelos de extração de tópicos são abordagens que visam descobrir padrões latentes nas relações entre os documentos e seus termos. Esses modelos se baseiam na premissa de que um documento é produzido a partir de tópicos previamente definidos que determinam os termos a serem utilizados em um documento. Nesse contexto, um documento é uma mistura de tópicos onde cada termo presente no documento pode ser associado a um tópico. Um tópico por sua vez, é uma estrutura com valor semântico representada por um conjunto de termos e seus pesos, os quais indicam o quão significante esses termos são para um assunto, e o quão útil pode ser para o  entendimento do tema do tópico~\cite{Steyvers2007,Blei2012}.



%        ==========|   Extração de Tópicos  |==========

Os modelos de extração de tópicos fornecem uma estratégia que visa encontrar nas relações entre documentos, padrões latentes que sejam significativos para o entendimento dessas relações~\cite{Wei2007}. Tais modelos podem ranquear um conjunto de termos importantes para um ou mais assuntos, bem como ranquear documentos por sua relevância para determinado tema~\cite{Faleiros2016,Xing2009}.
Atualmente, destacam-se os modelos probabilísticos de extração de tópicos como LDA~\cite{Blei2003} e PLSA~\cite{Hofmann1999}. São abordagens amplamente utilizadas ~\cite{DZhu20122} e frequentemente referenciadas em trabalhos que buscam extrair conhecimento e organizar bases textuais ~\cite{Aggarwal2018, OCallaghan2015, Steyvers2007}.  
%
%
Nesse trabalho, a expressão \textit{tópico} é usada para designar um assunto considerando que o mesmo foi extraído por meio de técnicas automáticas, ficando a expressão \textit{assunto} utilizada como seu teor popular. 

O processo de extração de tópicos atribui um peso a cada documento-tópico e uma relação termo-tópico que pode representar a probabilidade de ocorrência de um termo em um documento dado que o tópico está presente. A partir dessas representações, é possível agrupar documentos que compartilham o mesmo tópico bem como os termos que melhor descrevem o tópico~\cite{Aggarwal2018}. Com isso, obtém-se uma organização da coleção de documentos que favorece técnicas para navegação e consulta à coleção de documentos~\cite{Maracini2010}. 
% 
Além disso, essas abordagens de extração de tópicos fornecem a construção de novos atributos que representam os principais tópicos ou assuntos identificados na coleção de documentos, sendo uma oportunidade de incorporar conhecimento de domínio aos dados~\cite{Guyon2003}. 
% com o mesmo teor 


Para extrair esses tópicos, algumas técnicas foram propostas. Em termos de metodologia, a maioria dos trabalho enquadram-se em duas principais categorias, os modelos não-probabilísticos e os modelos probabilísticos.


\subsection{Modelos Não Probabilísticos}

Nos modelos não-probabilísticos a matriz documento-termo é projetada em um espaço com menor dimensionalidade chamado \textit{Latent Semantic Space}. 
Seja
$d \in D = \{d_1,\cdots,d_n\}$ o vetor que representa a coleção de documentos, 
$t \in T = \{t_1,\cdots,t_m\}$ seus termos distintos e 
$z \in Z = \{z_1,\cdots,z_k\}$ seus tópicos. 
Esses métodos aprendem decompondo a matriz documento-termo $W$, em duas matrizes $Z$ e $A$, tal que a resultante de $ZA$ seja uma aproximação da matriz $W$ original. Mais formalmente tem-se:

\begin{equation}
	Z\cdot A = \hat{W} \approx W
\end{equation}

Sendo $n$ o número de termos, $m$ o número de documentos da coleção, $k$ a quantidade de tópicos a serem extraídos, a matriz $A$ corresponde a matriz documento-tópico e possui dimensão $k \times n$. $Z$ corresponde a matriz termo-tópico e possui dimensão $m \times k$. Uma vez que $k \ll n,m$, então $A$ e $Z$ são menores que a matriz de entrada, o que resulta em uma versão comprimida da matriz original, pois $k \cdot n + m \cdot k \ll n \cdot m$. Ao final, obtém-se uma representação documento-tópico que atribui um peso para cada tópico em cada documento da coleção e uma representação termo-tópico que representa a probabilidade de ocorrência de um termo em um documento dado que o tópico está presente no documento.

Nesse sentido, o \textit{Latente Semantic Indexing} (LSA)~\cite{Deerwester1990} usa a técnica chamada \textit{Singular Value Decomposition} (SVD) para encontrar padrões no relacionamento entre assuntos e termos em uma coleção de texto não estruturada. Entretanto, esse método não fornece uma interpretação para elementos com valores negativos~\cite{Deerwester1990}~\cite{Cheng2013}. % Trocar essa referência do Cheng2013 pela que ele usa na seção 2 do trabalho dele.

% -- NMF
Outro modelo popular é o \textit{Non-Negative Matrix Factorization} (NMF)~\cite{Lee1999}.  
% Diferente do LSA, no processo de fatoração apenas operações aditivas são permitidas, o que garante que 
as matrizes resultantes não possuem elementos negativos, permitindo uma interpretação mais intuitiva de seus valores. O processo de fatoração proporciona o agrupamento das colunas da matriz $W$ o que possibilita, a propriedade \textit{clustering} a esse modelo.


\subsection{Modelos Probabilísticos}

Os modelos probabilísticos consideram os documentos como uma mistura de tópicos e um tópico como uma distribuição probabilística sobre os termos. O processo de elaboração do documento a partir desses tópicos é chamado de processo generativo ou modelo generativo, o qual é desconhecido, porém, pode ser estimado com base nos termos presentes no documento, também chamados de variáveis observáveis. Assim, o processo de extração de tópicos consiste em estimar o modelo generativo que deu origem aos documentos de uma coleção.
% falar em algum ponto sobre o problema das matrizes esparsas. Principalmente com documentos pequenos
 
% -- PLSA
O PLSA~\cite{Hofmann1999} foi um dos primeiros a estender o modelo LSA e formalizar a extração de tópicos probabilísticos. De maneira similar ao LSA, esse modelo decompõe uma matriz esparsa a fim de reduzir a dimensionalidade. O PLSA cria um modelo estatístico chamado \textit{aspect model} que associa os tópicos às variáveis observáveis atribuindo probabilidades às ligações entre os tópicos e os documentos e entre as palavras e os tópicos. Assim, cada documento pode ser representado como a probabilidade de um tópico estar presente, $P(z|d)$. E a probabilidade de um termo ocorrer dado que um tópico esta presente, $P(t|z)$. Em comparação ao LSA, é considerado um método mais robusto por proporcionar uma interpretação probabilística. Por outro lado, esse modelo apresenta desvantagens como o número de parâmetros do modelo que cresce linearmente com o número de documentos da coleção, o que pode ocasionar \textit{overfitting}.   % - E o Expectation Maximization?

% -- LDA
% A fim de contornar esses problemas, 
O LDA~\cite{Blei2003} estende o modelo PLSA incorporando um modelo generativo onde  cada tópico obedece à distribuição multivariada de \textit{Dirichlet} o que o torna menos propenso ao \textit{overfitting} e capaz de inferir tópicos a documentos ainda não observados. É referenciado na literatura como estado-da-arte sobre modelos probabilísticos de extração de tópicos e influencia uma grande quantidade de trabalhos, tornando-se base para novos modelos. 










O LDA utiliza a distribuição de Dirichlet para amostrar a distribuição dos tópicos. O modelo aloca os tópicos latentes que são distribuídos conforme a distribuição de Dirichlet. A função de densidade dessa distribuição é dada por:


\begin{equation}
Dir(z, \alpha) = \frac{1}{B(\alpha)} \prod_{k=1}^K z_k^{\alpha_k-1} , 
\end{equation}

onde $z = (z_1,\dots, z_K )$ e $\alpha = (\alpha_1,\dots,\alpha_K)$  são variáveis K-dimensionais e $B(\alpha)$ é a função Beta dada por:

\begin{equation}
	B(\alpha) = \frac{\prod_{k=1}^K \Gamma(\alpha_k)}{\Gamma(\sum_{k=1}^K \alpha_k)}
\end{equation} 



No modelo LDA, o processo de geração de palavras é o resultado da amostragem da Dirichlet é usado para atribuir as palavras de diferentes tópicos e que irão compor os documentos. 
%
Os tópicos são entendidos como distribuições probabilísticas sobre um vocabulário de palavras. Enquanto que os documentos, surgem da escolha aleatória das palavras presentes a uma distribuição de tópicos.
%
O processo gerador de um documento $d_j$ no modelo LDA pode ser detalhado como a seguir: 

\begin{enumerate}
	\item Crie as distribuições $\phi_k \backsim Dir(\phi_k, \beta)$ para cada tópico $k$;
	\item Crie uma distribuição $\theta_j \backsim Dir(theta_j,\alpha)$ para $d_j$;
	\item Escolha elemento $i$ a compor o documento $d_j$,
		\begin{enumerate}
			\item Atribua aleatoriamente um tópico $z_{j,i}\backsim Multinomial(\theta_j)$;
			\item Atribua aleatoriamente uma palavra $w_{j,i}$ com probabilidade $p(w_{j,i}|\phi_{z_{j,i}})$.
		\end{enumerate}
	
\end{enumerate}


%O processo gerador do modelo LDA, utiliza duas variáveis para representar as distribuições.
O processo gerador representa as distribuições por meio de duas variáveis. A variável $n$-dimensional $\phi$ em que $n$ é o número fixo de palavras do vocabulário, e a variável $K$-dimensional $\theta$. Essas variáveis são geradas por $Dir$ com seus respectivos parâmetros $\beta$ e $\alpha$.

Após as variáveis $\phi_k$ e $\theta_j$ serem inicializadas, gera-se por fim o documento $d_j$.
%
Assim cada documento é associado a múltiplos tópicos com proporções distintas e cada palavra do documento é obtida de um tópico específico que foi anteriormente obtido a partir da distribuição de tópicos do documento. Isso permite ao modelo LDA atribuir, para cada documento, múltiplos tópicos com proporções distintas~\cite{Blei2012, Faleiros2016}.




% -- K-Means





Outro modelo empregado na tarefa de extração de tópicos é o \textit{K-Means}. O \textit{K-Means} é um dos algoritmos de agrupamento particional mais usados, tendo se tornado bastante popular em tarefas de recuperação de informação~\cite{Manning2008}. O agrupamento de documentos de uma coleção se dá inicialmente pela codificação de cada documento em um vetores em que seus elementos o representam. Em seguida usa-se medidas de distância para medir as similaridades entre os documentos da coleção.
A ideia por traz do \textit{K-Means} é definir $k$ documentos aleatórios para representar os grupos, chamados centroides, e atribuir cada documento ao centroide mais próximo. Os centroides são recalculados os grupos rearranjados iterativamente até que não haja mudanças significativas.
O funcionamento do \textit{K-Means} para agrupar um coleção de documentos em $k$ grupos pode ser definido com a seguir:
\begin{enumerate}
	\item Selecione $k$ objetos iniciais, chamados centroides;
	\item Atribua cada documento ao grupo mais próximo;
	\item Calcule o novo centroide para cada grupo;
	\item Repita os passos 2 e 3 até que os grupos não sejam alterados significativamente.
\end{enumerate}

O critério de parada do \textit{K-Means} é determinado por soma dos erros quadráticos, definida como:

\begin{equation}
	E = \sum_{i=1}^k    \sum_{x \in G_i}     |x - l_i |^2~,
\end{equation}

\noindent
na qual $E$ é a somatória dos erros quadráticos calculados para cada documento da coleção, $x$ é o vetor de atributos que representa o documento, e $li$ é o centroide que representa o grupo $G_i$.

No contexto de extração de tópicos o \textit{K-Means} pode selecionar termos contidos na coleção a fim de descrever os grupos. Esses termos, chamados descritores podem ser selecionados pela frequência dos termos no centroide. 
%
Esses termos, chamados descritores podem ser atribuídos a cada grupo com base na frequência dos termos dos centroides. Verifica-se quais termos são mais comuns no documento que representa o centroide e que menos frequentes nos documentos mais distantes ao centroide~\cite{Pema2017, Bui2017, Rossi2011, Santos2010a, Manning2008}. 

Os modelos de extração de tópicos foram inicialmente propostos para utilização em Mineração de Texto onde são empregados na redução de dimensionalidade, extração de informações em textos, bem como na organização e recuperação de documentos, sendo utilizados para mensurar a relevância de um termo ou conjunto de termos para determinado assunto ou documento. Visto a popularidade nessas tarefas e flexibilidade dos modelos, logo notou-se sua utilidade em outros tipos de dados com atributos discretos como na genética, grafos e imagens. 


% \subsection{Pós-processamento}
